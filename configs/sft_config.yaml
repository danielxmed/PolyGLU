model:
  vocab_size: 151669
  d_model: 1024
  d_ff: 4096
  n_layers: 28
  n_q_heads: 16
  n_kv_heads: 8
  head_dim: 64
  seq_length: 4096
  n_activations: 4
  eps: 1.0e-6
  use_flash_attn: true

sft:
  # Optimizer
  peak_lr: 2.0e-5
  weight_decay: 0.1
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_eps: 1.0e-8
  grad_clip: 1.0

  # Schedule
  warmup_steps: 100
  epochs: 1

  # Tau (frozen at end of annealing)
  tau: 0.1

  # Batching
  micro_batch_size: 2
  grad_accum_steps: 16
  max_seq_length: 4096

  # Data
  dataset_id: nvidia/Nemotron-Math-v2
  dataset_splits:
    - high_part00
    - high_part01
    - high_part02

  # Logging
  wandb_project: polychromatic-lm-sft
  log_every: 50

  # Checkpointing
  checkpoint_dir: checkpoints_sft
  checkpoint_every: 500
  pretrained_checkpoint: checkpoints/portable_final.pt

  # DeepSpeed
  ds_config: configs/ds_config.json
